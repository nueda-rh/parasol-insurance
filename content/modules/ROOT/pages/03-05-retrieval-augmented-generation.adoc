=  RAG モデルの機能拡張
include::_attributes.adoc[]

LLMは非常に強力なツールですが、訓練データを通じて得られた知識や情報の範囲内でのみ機能します。しかし、訓練データに含まれていない質問をしなければならない場合はどうでしょう？または、訓練データには含まれていないが、それに関連する質問をしなければならない場合はどうでしょう？

この問題を解決する方法はいくつかあり、持っているリソースやそれにかけられる時間やお金によって異なります。以下にその例を示します：

- 必要な情報を含めるためにモデルを完全に再学習する。LLMに関しては、数百から数千のGPUを何週間も稼働させることができる限られた企業にしか不可能です。
- 新しい情報でモデルをファインチューニングする。ファインチューニングの場合必要なリソースは大きく減り、数日または数時間で行うことができます（モデルのサイズに依存）。ただし、モデルを完全に再学習しないため、新しい情報が回答に完全に取り込まれない可能性があります。ファインチューニングは特定のコンテキストや語彙の理解を向上させるのに優れています。頻繁に情報の更新がある場合、その度にモデルをファインチューニングし、デプロイし直す必要があります。
- 新しい情報をデータベースに保存し、クエリに関連する文章を検索して、その結果をLLMに送信するコンテキストとして追加します。この技術は**Retrieval Augmented Generation（RAG）**と呼ばれています。この方法は、モデルを再学習したりファインチューニングしたりせずに、新しい知識を利用でき、いつでも簡単に更新できます。

私たちはすでに[Milvus](https://milvus.io/)を使用してベクトルデータベースを準備しており、[California Driver's Handbook](https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/)（カリフォルニアの運転者向けハンドブック）の内容を[エンべディング](https://www.ibm.com/topics/embedding)して保存しています。

この演習では、RAGテクニックを使用して**請求に関するいくつかのクエリを実行**し、LLMを修正することなく、この新しい知識がどのように役立つかを確認します。

`parasol-insurance/lab-materials/03` フォルダから、 `03-05-retrieval-augmented-generation.ipynb` というノートブックを開き、指示に従ってください。

完了したら、ノートブックを閉じて次のページに進みます。
