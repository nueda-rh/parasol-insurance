{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## 異なるモデルの比較\n",
    "\n",
    "このノートブックでは、これまで利用してきたelyza/Llama-3-ELYZA-JP-8Bと並行して別のモデルであるMistral-7Bを使用し、その挙動を観察します。\n",
    "Mistral-7Bは日本語だけでなく多言語対応したモデルです。両者のアウトプットの違いを確認してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 必要なライブラリとインポート\n",
    "\n",
    "Labの指示に従って適切なワークベンチイメージを選択して起動した場合、必要なすべてのライブラリがすでにインストールされているはずです。もしインストールされていない場合は、次のセルの最初の行のコメントを外して正しいパッケージをすべてインストールしてください。その後、必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # 正しいワークベンチイメージを選択していない場合のみコメントを外してください\n",
    "\n",
    "import json\n",
    "import os\n",
    "import httpx\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchainパイプライン\n",
    "\n",
    "Langchainを使用して、パイプラインを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM推論APIのURL\n",
    "inference_server_url = \"_INFERENCE_URL_LLM_\"\n",
    "\n",
    "# LLMの定義\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"elyza\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    async_client=httpx.AsyncClient(verify=False),\n",
    "    http_client=httpx.Client(verify=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782046b7-f0a4-487c-86fc-3131e668c6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mistral LLM推論サーバーURL\n",
    "inference_server_url_mistral = \"http://llm.ic-shared-llm.svc.cluster.local:8000\"\n",
    "\n",
    "# LLMの定義\n",
    "llm_mistral = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base= f\"{inference_server_url_mistral}/v1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "それぞれのモデルに合わせた**テンプレート**を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template_string = \"\"\"\n",
    "あなたは、親切で、礼儀正しく、正直なアシスタントです。\n",
    "常に気配りと尊重をもって接し、真摯にサポートします。できる限り有用な返答を提供しますが、安全を確保します。\n",
    "有害で、倫理に反する、偏見のある、または否定的な内容は避けます。返答が公正でポジティブなものであることを確認します。\n",
    "\"\"\"\n",
    "\n",
    "user_template_string = \"\"\"\n",
    "与えられた文章の内容をもとに、与えられた質問に答えてください。\n",
    "\n",
    "### 文章:\n",
    "{text}\n",
    "\n",
    "### 質問:\n",
    "{query}\n",
    "\n",
    "### 回答:\n",
    "\"\"\"\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(system_template_string)\n",
    "user_template = HumanMessagePromptTemplate.from_template(user_template_string)\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([system_template, user_template])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]\n",
    "あなたは、親切で、礼儀正しく、正直なアシスタントです。\n",
    "常に気配りと尊重をもって接し、真摯にサポートします。できる限り有用な返答を提供しますが、安全を確保します。\n",
    "有害で、倫理に反する、偏見のある、または否定的な内容は避けます。返答が公正でポジティブなものであることを確認します。\n",
    "\n",
    "### テキスト:\n",
    "{text}\n",
    "\n",
    "### 質問:\n",
    "{query}\n",
    "\n",
    "### 回答:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT_MISTRAL = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "2つの**会話**オブジェクトを作成し、それぞれのモデルにクエリを投げる準備が行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )\n",
    "conversation_mistral = LLMChain(llm=llm_mistral,\n",
    "                        prompt=PROMPT_MISTRAL,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "モデルにクエリを投げる準備が整いました！\n",
    "\n",
    "この例では、1つの請求文章を対象にクエリを実行しどのような結果が得られるかを見てみます。もちろん、他の請求文章で試してみても大丈夫です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-95e5-b12a5a1b1878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'claims/claim1.json'\n",
    "\n",
    "# Opening JSON file\n",
    "claims = {}\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claims[filename] = data\n",
    "\n",
    "# Analyze the claim\n",
    "print(f\"***************************\")\n",
    "print(f\"* 請求: {filename}\")\n",
    "print(f\"***************************\")\n",
    "print(\"元の文章:\")\n",
    "print(\"-----------------\")\n",
    "print(f\"件名: {claims[filename]['subject']}\\n内容:\\n{claims[filename]['content']}\\n\\n\")\n",
    "print('Elyzaによる分析:')\n",
    "print(\"--------\")\n",
    "text_input = f\"件名: {claims[filename]['subject']}\\n内容:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"この請求の文章から読み取れる感情はどのようなものですか？「肯定的」、「否定的」、「どちらでもない」から1つだけ選んで答え、その理由もあわせて説明してください。\"\n",
    "location_query = \"この請求に関連する出来事はどこで起こりましたか？出来事の発生した場所について、市区町村や通りの名前などを答えて下さい。\"\n",
    "time_query = \"この請求に関連する出来事はいつ起こりましたか？日付と、時刻あるいは時間帯を一言で答えて下さい。\"\n",
    "print(f\"- 送信者の感情: \")\n",
    "conversation.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- 発生場所: \")\n",
    "conversation.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- 発生日時: \")\n",
    "conversation.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")\n",
    "print('Mistralによる分析:')\n",
    "print(\"--------\")\n",
    "text_input = f\"件名: {claims[filename]['subject']}\\n内容:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"この請求の文章から読み取れる感情はどのようなものですか？「肯定的」、「否定的」、「どちらでもない」から1つだけ選んで答え、その理由もあわせて説明してください。\"\n",
    "location_query = \"この請求に関連する出来事はどこで起こりましたか？出来事の発生した場所について、市区町村や通りの名前などを答えて下さい。\"\n",
    "time_query = \"この請求に関連する出来事はいつ起こりましたか？日付と、時刻あるいは時間帯を一言で答えて下さい。\"\n",
    "print(f\"- 送信者の感情: \")\n",
    "conversation_mistral.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- 発生場所: \")\n",
    "conversation_mistral.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- 発生日時: \")\n",
    "conversation_mistral.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b21a9-86b3-4f29-87cf-f9839b4ee300",
   "metadata": {},
   "source": [
    "両者の出力はどちらも正しいですが、日本語に特化したチューニングが行われたElyzaの方がより自然な回答となっています。\n",
    "\n",
    "LLMを扱う際の技術は、求めるパフォーマンスと精度の間、ならびにそれに伴うリソースやコストとのバランスを見つけることです。\n",
    "\n",
    "そのため、データが変化したり、モデルが進化したりする際に、常に期待通りの挙動が得られるようにするための信頼性チェックを行うことが重要となります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
