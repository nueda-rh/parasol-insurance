{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
      "metadata": {},
      "source": [
       "## タスクごとのモデル比較\n",
       "\n",
       "このノートブックでは、Mistral-7Bと並行して別のモデルであるFlan-T5-largeを使用し、その動作を確認します。\n",
       "\n",
       "Flan-T5-Largeは確かに小型で、GPUを使用せず、4GBのRAMだけで動作しますが、タスクに対応できるでしょうか？"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
      "metadata": {},
      "source": [
       "### 必要なものとインポート\n",
       "\n",
       "もしLabの指示通りに適切なワークベンチイメージを選択して起動していれば、必要なライブラリはすでに揃っているはずです。そうでない場合は、次のセルの最初の行をアンコメントして、すべての必要なパッケージをインストールしてください。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # 適切なワークベンチイメージを選択していない場合のみアンコメントしてください\n",
       "\n",
       "import json\n",
       "import os\n",
       "from os import listdir\n",
       "from os.path import isfile, join\n",
       "from langchain.chains import LLMChain\n",
       "from langchain_community.llms import VLLMOpenAI, HuggingFaceTextGenInference\n",
       "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
       "from langchain.prompts import PromptTemplate"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c428fbad-2345-4536-b687-72416d6b9b15",
      "metadata": {},
      "source": [
       "### Langchainパイプライン\n",
       "\n",
       "ここで、2つの異なるLLMエンドポイントと2つの異なるLangchainパイプラインを定義します。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "# LLM推論サーバーURL\n",
       "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:8000\"\n",
       "\n",
       "# LLM定義\n",
       "llm = VLLMOpenAI(           # vLLMのOpenAI互換APIクライアントを使用しますが、モデルはOpenShift上で動作しており、OpenAIではありません。\n",
       "    openai_api_key=\"EMPTY\",   # そのため、OpenAIキーは不要です。\n",
       "    openai_api_base= f\"{inference_server_url}/v1\",\n",
       "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
       "    top_p=0.92,\n",
       "    temperature=0.01,\n",
       "    max_tokens=512,\n",
       "    presence_penalty=1.03,\n",
       "    streaming=True,\n",
       "    callbacks=[StreamingStdOutCallbackHandler()]\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "782046b7-f0a4-487c-86fc-3131e668c6c0",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "import warnings\n",
       "warnings.filterwarnings(\"ignore\")\n",
       "\n",
       "# Flan-T5-Small LLM推論サーバーURL\n",
       "inference_server_url_flan_t5 = \"http://llm-flant5.ic-shared-llm.svc.cluster.local:3000/\"\n",
       "\n",
       "# LLM定義\n",
       "llm_flant5 = HuggingFaceTextGenInference(\n",
       "    inference_server_url=inference_server_url_flan_t5,\n",
       "    max_new_tokens=96,\n",
       "    top_k=10,\n",
       "    top_p=0.95,\n",
       "    typical_p=0.95,\n",
       "    temperature=0.01,\n",
       "    repetition_penalty=1.03,\n",
       "    streaming=True,\n",
       "    callbacks=[StreamingStdOutCallbackHandler()]\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
      "metadata": {},
      "source": [
       "両モデルに対する**テンプレート**は同じものを使用します。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "template=\"\"\"<s>[INST]\n",
       "あなたは助けになる、丁寧で正直なアシスタントです。\n",
       "常に慎重に、尊敬と真実をもって対応してください。\n",
       "有用でありつつも、安全な応答を心がけてください。\n",
       "有害、非倫理的、偏見のある、または否定的な内容を避け、公正さと前向きな応答を心がけてください。\n",
       "私はテキストを提示し、その後に質問をします。この質問に対して、できる限り簡潔かつ正確に答えてください。\n",
       "\n",
       "### テキスト:\n",
       "{text}\n",
       "\n",
       "### 質問:\n",
       "{query}\n",
       "\n",
       "### 答え:\n",
       "[/INST]\n",
       "\"\"\"\n",
       "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
      "metadata": {},
      "source": [
       "これで、2つの**会話**オブジェクトを作成し、それらを使用して2つのモデルにクエリを送ることができます。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "conversation = LLMChain(llm=llm,\n",
       "                        prompt=PROMPT,\n",
       "                        verbose=False\n",
       "                        )\n",
       "conversation_flant5 = LLMChain(llm=llm_flant5,\n",
       "                        prompt=PROMPT,\n",
       "                        verbose=False\n",
       "                        )"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
      "metadata": {},
      "source": [
       "さあ、モデルにクエリを送る準備が整いました！\n",
       "\n",
       "この例では、1つのクレームに対してのみクエリを送り、その結果を確認します。もちろん、異なるクレームで試してみても構いません。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac009d5-d558-4258-84fe-91e399d8f65c",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "# クレーム\n",
       "text = \"蜂蜜は、最も古い自然の甘味料であると考えられています。\"\n",
       "\n",
       "# 質問\n",
       "query = \"蜂蜜が最も古い自然の甘味料であると考えられている理由は何ですか？\"\n",
       "\n",
       "# 両モデルへのクエリ\n",
       "conversation.run(text=text, query=query)\n",
       "conversation_flant5.run(text=text, query=query)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.9.16"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   