{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## 異なるモデルの比較\n",
    "\n",
    "このノートブックでは、これまで利用してきたELYZA-japanese-Llama-2-7b-fast-instructと並行して別のモデルであるFlan-T5-largeを使用し、その挙動を観察します。\n",
    "\n",
    "Flan-T5-Largeは小型で、GPUなしで動作し、4GBのRAMしか使用しませんが、要求されたタスクをこなすことができるのでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 必要なライブラリとインポート\n",
    "\n",
    "Labの指示に従って適切なワークベンチイメージを選択して起動した場合、必要なすべてのライブラリがすでにインストールされているはずです。もしインストールされていない場合は、次のセルの最初の行のコメントを外して正しいパッケージをすべてインストールしてください。その後、必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # 正しいワークベンチイメージを選択していない場合のみコメントを外してください\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchainパイプライン\n",
    "\n",
    "Langchainを使用して、パイプラインを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM推論APIのURL\n",
    "inference_server_url = \"_INFERENCE_URL_LLM_\"\n",
    "\n",
    "# LLMの定義\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"elyza\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "782046b7-f0a4-487c-86fc-3131e668c6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flan-T5-Small LLM推論サーバーURL\n",
    "inference_server_url_flan_t5 = \"http://llm-flant5.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLMの定義\n",
    "llm_flant5 = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url_flan_t5,\n",
    "    max_new_tokens=96,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "両方のモデルに対して同じ**テンプレート**を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template_string = \"\"\"\n",
    "あなたは、親切で、礼儀正しく、正直なアシスタントです。\n",
    "常に気配りと尊重をもって接し、真摯にサポートします。できる限り有用な返答を提供しますが、安全を確保します。\n",
    "有害で、倫理に反する、偏見のある、または否定的な内容は避けます。返答が公正でポジティブなものであることを確認します。\n",
    "\"\"\"\n",
    "\n",
    "user_template_string = \"\"\"\n",
    "与えられた文章の内容をもとに、与えられた質問に答えてください。\n",
    "\n",
    "### 文章:\n",
    "{text}\n",
    "\n",
    "### 質問:\n",
    "{query}\n",
    "\n",
    "### 回答:\n",
    "\"\"\"\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(system_template_string)\n",
    "user_template = HumanMessagePromptTemplate.from_template(user_template_string)\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([system_template, user_template])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "2つの**会話**オブジェクトを作成し、それぞれのモデルにクエリを投げる準備が行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )\n",
    "conversation_flant5 = LLMChain(llm=llm_flant5,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "モデルにクエリを投げる準備が整いました！\n",
    "\n",
    "この例では、1つの請求文章を対象にクエリを実行しどのような結果が得られるかを見てみます。もちろん、他の請求文章で試してみても大丈夫です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-95e5-b12a5a1b1878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'claims/claim1.json'\n",
    "\n",
    "# Opening JSON file\n",
    "claims = {}\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claims[filename] = data\n",
    "\n",
    "# Analyze the claim\n",
    "print(f\"***************************\")\n",
    "print(f\"* 請求: {filename}\")\n",
    "print(f\"***************************\")\n",
    "print(\"元の文章:\")\n",
    "print(\"-----------------\")\n",
    "print(f\"件名: {claims[filename]['subject']}\\n内容:\\n{claims[filename]['content']}\\n\\n\")\n",
    "print('Elyzaによる分析:')\n",
    "print(\"--------\")\n",
    "text_input = f\"件名: {claims[filename]['subject']}\\n内容:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"この請求の文章から読み取れる感情はどのようなものですか？「肯定的」、「否定的」、「どちらでもない」から1つだけ選んで答え、その理由もあわせて説明してください。\"\n",
    "location_query = \"この請求に関連する出来事はどこで起こりましたか？出来事の発生した場所について、市区町村や通りの名前などを答えて下さい。\"\n",
    "time_query = \"この請求に関連する出来事はいつ起こりましたか？日付と、時刻あるいは時間帯を一言で答えて下さい。\"\n",
    "print(f\"- 送信者の感情: \")\n",
    "conversation.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- 発生場所: \")\n",
    "conversation.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- 発生日時: \")\n",
    "conversation.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")\n",
    "print('Flan-T5-Largeによる分析:')\n",
    "print(\"--------\")\n",
    "text_input = f\"件名: {claims[filename]['subject']}\\n内容:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"この請求の文章から読み取れる感情はどのようなものですか？「肯定的」、「否定的」、「どちらでもない」から1つだけ選んで答え、その理由もあわせて説明してください。\"\n",
    "location_query = \"この請求に関連する出来事はどこで起こりましたか？出来事の発生した場所について、市区町村や通りの名前などを答えて下さい。\"\n",
    "time_query = \"この請求に関連する出来事はいつ起こりましたか？日付と、時刻あるいは時間帯を一言で答えて下さい。\"\n",
    "print(f\"- 送信者の感情: \")\n",
    "conversation_flant5.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- 発生場所: \")\n",
    "conversation_flant5.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- 発生日時: \")\n",
    "conversation_flant5.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b21a9-86b3-4f29-87cf-f9839b4ee300",
   "metadata": {},
   "source": [
    "\n",
    "Flan-T5-Largeは770百万パラメータのモデルであるため、一部の結果を生成するのが速いです。しかし、その結果は精度や詳細性に欠けています。したがって、ある程度は機能しますが、70億パラメータのElyzaから得られる結果には及びません。\n",
    "\n",
    "LLMを扱う際の技術は、求めるパフォーマンスと精度の間、ならびにそれに伴うリソースやコストとのバランスを見つけることです。\n",
    "\n",
    "そのため、データが変化したり、モデルが進化したりする際に、常に期待通りの挙動が得られるようにするための信頼性チェックを行うことが重要となります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
