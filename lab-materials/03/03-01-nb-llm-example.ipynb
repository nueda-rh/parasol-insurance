{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## LLMをプログラムで操作する\n",
    "\n",
    "既に多くの方は、ChatGPTのような大規模言語モデル（LLM）とやり取りしたことがあるかもしれません。通常、これはUIやアプリケーションを介して行われます。\n",
    "\n",
    "このNotebookでは、Pythonを使用してLLMに直接アクセスします。\n",
    "モデルとして、OpenShift AIでServingされた[**elyza/Llama-3-ELYZA-JP-8B**](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)を利用します。\n",
    "このLLMはMeta社が開発したLlama3というモデルをベースに、日本のスタートアップであるElyza社が日本語データでチューニングを行ったモデルです。\n",
    "\n",
    "このモデルは既にラボクラスターにデプロイされています。小型モデルとはいえ、動作には24GBのRAMを持つGPUが必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 必要なライブラリとインポート\n",
    "\n",
    "Labの指示に従って適切なワークベンチイメージを選択して起動した場合、必要なすべてのライブラリがすでにインストールされているはずです。もしインストールされていない場合は、次のセルの最初の行のコメントを外して正しいパッケージをすべてインストールしてください。その後、必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # 正しいワークベンチイメージを選択していない場合のみ、コメントを外してください\n",
    "\n",
    "import json\n",
    "import os\n",
    "import httpx\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "[Langchain](https://www.langchain.com/)は、言語モデルを活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLM（Language Model）に適切にクエリを発行するために手動で書かなければならないすべてのコードを処理します。\n",
    "\n",
    "まず、LLMインスタンスを作成します。これはLLM APIへのクエリが行われる場所と、モデルに適用されるいくつかのパラメータによって定義されます。たとえば、`max_new_tokens`はモデルが最大512トークン（単語または単語の一部）で回答するよう指示します。`temperature`はここで非常に低く設定されており、モデルに真実に基づいたままであり、あまり「創造的」にならないように指示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM推論APIのURL\n",
    "inference_server_url = \"_INFERENCE_URL_LLM_\"\n",
    "\n",
    "# LLMの定義\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\", # OpenAI互換のAPIクライアントを使用していますが、モデルはOpenAIではなくOpenShift上で実行されています。そのため、api keyを指定しますが、これは使われません。\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"elyza\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    async_client=httpx.AsyncClient(verify=False),\n",
    "    http_client=httpx.Client(verify=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "また、モデルに送信するすべてのリクエストに適用する**テンプレート**（プロンプト）も必要です。\n",
    "\n",
    "モデルにクエリを送信する際、ほとんどの場合、ユーザーが入力した内容をそのまま送ることは望まれません。モデルがそれをどのように扱うかを正確に指示する必要があります。例えば、何をどのように回答するか、回答してはいけないこと、使用するべきトーンなどです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template_string = \"\"\"\n",
    "あなたは、親切で、礼儀正しく、正直なアシスタントです。\n",
    "常に気配りと尊重をもって接し、真摯にサポートします。できる限り有用な返答を提供しますが、安全を確保します。\n",
    "有害で、倫理に反する、偏見のある、または否定的な内容は避けます。返答が公正でポジティブなものであることを確認します。\n",
    "\"\"\"\n",
    "\n",
    "user_template_string = \"\"\"\n",
    "与えられた質問に対して、できるだけ多くの情報を含めて回答して下さい。\n",
    "\n",
    "### 質問:\n",
    "{input}\n",
    "\n",
    "### 回答:\n",
    "\"\"\"\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(system_template_string)\n",
    "user_template = HumanMessagePromptTemplate.from_template(user_template_string)\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([system_template, user_template])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "Langchainはこれらの要素を簡単に「つなぎ合わせ」、モデルにクエリを送信するために使用する**会話**オブジェクト (conversation) を作成することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "これでモデルにクエリを送信する準備が整いました！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"人工知能（AI）とは、どのようなものですか？\"\n",
    "\n",
    "conversation.predict(input=query); # 行末の \";\" は、最終の出力（ストリームされた回答の繰り返し）を非表示にします。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
