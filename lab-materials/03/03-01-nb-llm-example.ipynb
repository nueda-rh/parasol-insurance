{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## LLMをプログラムで操作する\n",
    "\n",
    "これまでにChatGPTのような大規模言語モデル（LLM）とインターフェースしたことがあると思います。通常は、UIやアプリケーションを通じて行います。\n",
    "\n",
    "このノートブックでは、Pythonを使用してLLMに接続し、APIを介して直接クエリを行います。このラボでは、モデルとして**Mistral-7B Instruct v2**（https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2）を選択しました。このモデルは完全にオープンソース（Apache 2.0ライセンス）で、他の商用やオープンソースモデルに比べて軽量ですが、我々が使用しようとしているタスクにおいて非常に高性能です。\n",
    "\n",
    "このモデルは既にラボクラスターにデプロイされています。小型モデルとはいえ、動作には24GBのRAMを持つGPUが必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 必要条件とインポート\n",
    "\n",
    "ラボの指示に従って正しいワークベンチイメージを選択した場合は、必要なライブラリがすでにインストールされています。もしそうでない場合は、次のセルの最初の行のコメントを外して、必要なパッケージをインストールしてください。続いて必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # 正しいワークベンチイメージを選択していない場合のみ、コメントを外してください\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain (https://www.langchain.com/) は、言語モデルを利用したアプリケーション開発のためのフレームワークです。これにより、LLMを適切にクエリするための面倒なボイラープレートコードを書く必要がなくなります。\n",
    "\n",
    "まず、LLMインスタンスを作成します。これはLLM APIがクエリできる場所と、モデルに適用されるいくつかのパラメータによって定義されます。例えば、`max_new_tokens`はモデルが最大512トークン（単語または単語の一部）まで回答するように指示します。`temperature`は非常に低く設定されており、モデルがあまり「クリエイティブ」にならないようにし、事実に基づいた回答を行うようにします。結局のところ、ここでは派手な詩を書こうとしているわけではありません！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM推論サーバーのURL\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:8000\"\n",
    "\n",
    "# LLMの定義\n",
    "llm = VLLMOpenAI(           # vLLMのOpenAI互換APIクライアントを使用しています。しかし、モデルはOpenShift上で実行されています。\n",
    "    openai_api_key=\"EMPTY\",   # したがって、OpenAIキーは必要ありません。\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "また、送信するリクエストごとに適用される**テンプレート**（「プロンプト」）も必要です。\n",
    "\n",
    "モデルにクエリを送る際、ユーザーが入力した内容をそのまま送ることはほとんどありません。この入力に加えて、モデルがどのようにそれを処理すべきか、回答方法や回答しない内容、使用する口調などの適切な指示をモデルに与える必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]<<SYS>>\n",
    "あなたは役立つ、尊敬を持った、誠実なアシスタントです。可能な限り有益であり、かつ安全であることが求められます。\n",
    "質問が行われますので、それに答えてください。\n",
    "回答には有害、非倫理的、人種差別的、性差別的、有毒、危険、または違法な内容を含めないでください。\n",
    "あなたの回答は社会的に偏見のないもので、前向きであるべきです。\n",
    "質問が意味不明である場合や、事実として一貫性がない場合は、誤った回答をするのではなく、その理由を説明してください。\n",
    "質問の答えがわからない場合は「わかりません」と答えてください。\n",
    "<</SYS>>\n",
    "\n",
    "### 質問:\n",
    "{input}\n",
    "\n",
    "### 回答:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "Langchainを使用すると、これらの要素を簡単に「結びつけ」て**会話**オブジェクトを作成し、モデルにクエリを送信することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "これでモデルにクエリを送信する準備が整いました！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"人工知能とは何ですか？\"\n",
    "\n",
    "conversation.predict(input=query); # 行の最後に\";\"を追加することで、最終出力（ストリームされた回答の繰り返し）を隠すことができます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089476-bba0-4093-8be8-1469780afaba",
   "metadata": {},
   "source": [
    "このノートブックのセクション3.7に戻ると、いくつかのオプション演習がありますので、試してみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
