{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## モデルの機能を拡張する\n",
    "\n",
    "LLMは非常に強力なツールですが、訓練データを通じて得られた知識や情報の範囲内でのみ機能します。しかし、訓練データに含まれていない質問をしなければならない場合はどうでしょう？または、訓練データには含まれていないが、それに関連する質問をしなければならない場合はどうでしょう？\n",
    "\n",
    "この問題を解決する方法はいくつかあり、持っているリソースやそれにかけられる時間やお金によって異なります。以下にその例を示します：\n",
    "\n",
    "- 必要な情報を含めるためにモデルを完全に再学習する。LLMに関しては、数百から数千のGPUを何週間も稼働させることができる限られた企業にしか不可能です。\n",
    "- 新しい情報でモデルをファインチューニングする。ファインチューニングの場合必要なリソースは大きく減り、数日または数時間で行うことができます（モデルのサイズに依存）。ただし、モデルを完全に再学習しないため、新しい情報が回答に完全に取り込まれない可能性があります。ファインチューニングは特定のコンテキストや語彙の理解を向上させるのに優れています。頻繁に情報の更新がある場合、その度にモデルをファインチューニングし、デプロイし直す必要があります。\n",
    "- 新しい情報をデータベースに保存し、クエリに関連する文章を検索して、その結果をLLMに送信するコンテキストとして追加します。この技術は**Retrieval Augmented Generation（RAG）**と呼ばれています。この方法は、モデルを再学習したりファインチューニングしたりせずに、新しい知識を利用でき、いつでも簡単に更新できます。\n",
    "\n",
    "私たちはすでに[Milvus](https://milvus.io/)を使用してベクトルデータベースを準備しており、[California Driver's Handbook](https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/)（カリフォルニアの運転者向けハンドブック）の内容を[エンべディング](https://www.ibm.com/topics/embedding)して保存しています。\n",
    "\n",
    "このノートブックでは、RAGを使用して**請求文章に関するいくつかのクエリを行い**、追加された知識がどのように役立つかを確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 必要なライブラリとインポート\n",
    "\n",
    "Labの指示に従って適切なワークベンチイメージを選択して起動した場合、必要なすべてのライブラリがすでにインストールされているはずです。もしインストールされていない場合は、次のセルの最初の行のコメントを外して正しいパッケージをすべてインストールしてください。その後、必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # 正しいワークベンチイメージを選択していない場合のみコメントを外してください\n",
    "\n",
    "import json\n",
    "import os\n",
    "import httpx\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import RetrievalQA, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "from milvus_retriever_with_score_threshold import MilvusRetrieverWithScoreThreshold\n",
    "\n",
    "# 埋め込みモデルをダウンロードする際の警告を無効にする\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchainの設定\n",
    "\n",
    "Langchainを使用してタスクパイプラインを定義します。\n",
    "\n",
    "まず、クエリを送信する**LLM**を定義しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM推論APIのURL\n",
    "inference_server_url = \"_INFERENCE_URL_LLM_\"\n",
    "\n",
    "# LLMの定義\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"elyza\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    async_client=httpx.AsyncClient(verify=False),\n",
    "    http_client=httpx.Client(verify=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13907-14f1-4995-9756-8778c19a2101",
   "metadata": {},
   "source": [
    "次に、California Driver Handbookを準備して保存している**ベクトルデータベース**への接続を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849c1a0-7fe5-425f-853d-6a9e67a38971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 最初に、ハンドブックを処理するために使用したエンべディングを定義します\n",
    "model_kwargs = {\"trust_remote_code\": True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "            model_kwargs=model_kwargs,\n",
    "            show_progress=False,\n",
    "        )\n",
    "\n",
    "# 次に、Milvusベクトルデータベースから関連データを取得するためのリトリーバーを定義します\n",
    "retriever = MilvusRetrieverWithScoreThreshold(\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"california_driver_handbook_1_0\",\n",
    "            collection_description=\"\",\n",
    "            collection_properties=None,\n",
    "            connection_args={\n",
    "                \"host\": \"vectordb-milvus.ic-shared-milvus.svc.cluster.local\",\n",
    "                \"port\": \"19530\",\n",
    "                \"user\": \"root\",\n",
    "                \"password\": \"Milvus\",\n",
    "            },\n",
    "            consistency_level=\"Session\",\n",
    "            search_params=None,\n",
    "            k=4,\n",
    "            score_threshold=0.99,\n",
    "            metadata_field=\"metadata\",\n",
    "            text_field=\"page_content\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055b55e-dc39-4ca2-b5e1-07b2e98f67c6",
   "metadata": {},
   "source": [
    "次に、クエリを作成するために使用するテンプレートを定義します。この**テンプレート**には、**References**（参照）セクションが含まれていることに注意してください。ベクトルデータベースから返されたドキュメントが挿入されるのはこのセクションです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfdce1-a873-4c43-9c79-05c6f3cf7f41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template_string = \"\"\"\n",
    "あなたは、親切で、礼儀正しく、正直なアシスタントです。\n",
    "常に気配りと尊重をもって接し、真摯にサポートします。できる限り有用な返答を提供しますが、安全を確保します。\n",
    "有害で、倫理に反する、偏見のある、または否定的な内容は避けます。返答が公正でポジティブなものであることを確認します。\n",
    "\"\"\"\n",
    "\n",
    "user_template_string = \"\"\"\n",
    "与えられた文章と参照情報の内容をもとに、与えられた質問に答えてください。\n",
    "\n",
    "### 文章:\n",
    "{text}\n",
    "\n",
    "### 参照情報:\n",
    "{context}\n",
    "\n",
    "参照して得られた情報は日本語に変換して下さい。\n",
    "\n",
    "### 質問:\n",
    "{query}\n",
    "\n",
    "回答は日本語で実施して下さい。\n",
    "\n",
    "### 回答:\n",
    "\"\"\"\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(system_template_string)\n",
    "user_template = HumanMessagePromptTemplate.from_template(user_template_string)\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([system_template, user_template])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a18d5c-4173-4fc6-bbf2-f61cc847f2d5",
   "metadata": {},
   "source": [
    "モデルに問い合わせる準備ができました！\n",
    "claimsフォルダには、請求文章の例が記載されたJSONファイルがあります。最初の請求文章を読み込み、それに関連する質問をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9c4c0-5017-4eaf-9d19-29fa2e63cfbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 請求文章の読み取り\n",
    "\n",
    "filename = 'claims/claim1.json'\n",
    "\n",
    "# JSONファイルを開く\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claim = data[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50f94d-8cd1-4d5e-b50e-0fcabfe3507d",
   "metadata": {},
   "source": [
    "## LLMの実行、追加知識なし\n",
    "まずはこれまで通りベクトルデータベースの助けなしに、請求文章に関する最初のクエリを実行しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aebaa4-59e8-4380-b8e3-b0ab637e2823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# クエリを作成して送信します。\n",
    "\n",
    "query = \"ダニエルは赤信号で通過することを許可されていましたか？\"\n",
    "\n",
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )\n",
    "resp = conversation.predict(context=\"\", query=query, text=claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f714d-c6e7-4220-a16b-fc65dbae91fb",
   "metadata": {},
   "source": [
    "モデルが一般常識として交通規制に関しての理解があることを確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e9a93-9b81-424a-96a9-f447e417c8c1",
   "metadata": {},
   "source": [
    "### ## LLMの実行、追加知識あり\n",
    "\n",
    "同じプロンプトとクエリを使用しますが、RAGによりモデルがCalifornia Driver Handbookにアクセスできるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# クエリを作成して送信します。\n",
    "\n",
    "query = \"ダニエルは赤信号で通過することを許可されていましたか？\"\n",
    "search_query = \"Was Daniel allowed to pass at the red light?\"\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "resp = chain.invoke({\"query\": query,\"input\": search_query, \"text\": claim})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5659f0-a27f-4b9e-8dd1-e05f37671c8f",
   "metadata": {},
   "source": [
    "モデルは、**赤信号は「停止」を意味する**という情報源を直接参照することができました。\n",
    "\n",
    "情報の取得元についてベクトルデータベースの回答に関連するソースを確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e424c52-9a1a-4425-a105-4e0744ec0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sources(input_list):\n",
    "        sources = \"\"\n",
    "        if len(input_list) != 0:\n",
    "            sources += input_list[0].metadata[\"source\"] + ', page: ' + str(input_list[0].metadata[\"page\"])\n",
    "            page_list = [input_list[0].metadata[\"page\"]]\n",
    "            for item in input_list:\n",
    "                if item.metadata[\"page\"] not in page_list: # 重複を避ける\n",
    "                    page_list.append(item.metadata[\"page\"])\n",
    "                    sources += ', ' + str(item.metadata[\"page\"])\n",
    "        return sources\n",
    "\n",
    "results = format_sources(resp['context'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8cd32-0bdb-484d-a8bd-fb108ce2f131",
   "metadata": {},
   "source": [
    "これで完了です！私たちは、外部知識を使ってLLMを補完する方法を学びました！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
