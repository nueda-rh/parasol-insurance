{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
      "metadata": {
       "tags": []
      },
      "source": [
       "## モデルの能力を拡張する\n",
       "\n",
       "大規模言語モデル (LLM) は非常に強力なツールですが、訓練された知識や情報の範囲内でしかその力を発揮できません。結局のところ、知っていることしか答えられないということです。しかし、もし訓練データに含まれていない質問をしたい場合はどうでしょうか？また、訓練データに含まれていないが、関連する質問をしたい場合は？\n",
       "\n",
       "この問題を解決する方法はいくつかありますが、使えるリソースや、費やせる時間や費用に応じて異なります。以下はそのいくつかのオプションです。\n",
       "\n",
       "- 必要な情報を含むようにモデルを完全に再訓練する。これは、大規模言語モデルの場合、数千のGPUを数週間にわたって稼働させることができる数社のみが可能です。\n",
       "- この新しい情報を使ってモデルを微調整する。これにははるかに少ないリソースが必要で、通常は数時間または数分で完了します（モデルのサイズによります）。ただし、完全に再訓練するわけではないため、新しい情報が回答に完全に統合されない場合があります。微調整は特定のコンテキストや語彙をよりよく理解するのに優れており、新しい知識を注入する点では少し劣ります。また、新しい情報を追加するたびに再訓練と再デプロイが必要です。\n",
       "- 新しい情報をデータベースに格納し、クエリに関連する部分をクエリのコンテキストとして取り出してから、LLMに送信する。この技術は**検索強化生成 (Retrieval Augmented Generation, RAG)** と呼ばれます。再訓練や微調整を必要とせず、いつでも簡単に更新可能な新しい知識を活用できる点で興味深い技術です。\n",
       "\n",
       "すでに [Milvus](https://milvus.io/) を使用してベクターデータベースを準備し、[埋め込み](https://www.ibm.com/topics/embedding)形式で[カリフォルニア運転手ハンドブック](https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/)の内容を保存しています。\n",
       "\n",
       "このノートブックでは、RAG を使用して**ある請求に関するクエリ**を行い、この新しい知識がどのように役立つかを、モデルを変更せずに確認します。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
      "metadata": {},
      "source": [
       "### 必要なものとインポート\n",
       "\n",
       "Lab の指示に従って適切なワークベンチイメージを選択した場合は、すでに必要なライブラリがすべて揃っているはずです。もしそうでない場合は、次のセルの最初の行のコメントを解除して、正しいパッケージをインストールしてください。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # 適切なワークベンチイメージを選択していない場合のみコメントを解除\n",
       "\n",
       "import json\n",
       "import os\n",
       "from os import listdir\n",
       "from os.path import isfile, join\n",
       "from langchain.chains import LLMChain\n",
       "from langchain.chains import LLMChain, RetrievalQA\n",
       "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
       "from langchain_community.llms import VLLMOpenAI\n",
       "from langchain_community.vectorstores import Milvus\n",
       "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
       "from langchain.prompts import PromptTemplate\n",
       "from milvus_retriever_with_score_threshold import MilvusRetrieverWithScoreThreshold\n",
       "\n",
       "# 埋め込みモデルをダウンロードする際の警告をオフにする\n",
       "import transformers\n",
       "transformers.logging.set_verbosity_error()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c428fbad-2345-4536-b687-72416d6b9b15",
      "metadata": {},
      "source": [
       "### Langchain の要素\n",
       "\n",
       "再び、Langchain を使ってタスクパイプラインを定義します。\n",
       "\n",
       "まずは、クエリを送信するための **LLM** です。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "# LLM 推論サーバーの URL\n",
       "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:8000\"\n",
       "\n",
       "# LLM の定義\n",
       "llm = VLLMOpenAI(           # vLLM OpenAI 互換の API クライアントを使用します。ただし、モデルは OpenShift 上で稼働しており、OpenAI 上ではありません。\n",
       "    openai_api_key=\"EMPTY\",   # そのため、OpenAI のキーは不要です。\n",
       "    openai_api_base= f\"{inference_server_url}/v1\",\n",
       "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
       "    top_p=0.92,\n",
       "    temperature=0.01,\n",
       "    max_tokens=512,\n",
       "    presence_penalty=1.03,\n",
       "    streaming=True,\n",
       "    callbacks=[StreamingStdOutCallbackHandler()]\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "4fa13907-14f1-4995-9756-8778c19a2101",
      "metadata": {},
      "source": [
       "次に、カリフォルニア運転手ハンドブックを準備し保存した **ベクターデータベース** への接続です。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f849c1a0-7fe5-425f-853d-6a9e67a38971",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "# まず、ハンドブックの処理に使用した埋め込みを定義します\n",
       "model_kwargs = {\"trust_remote_code\": True}\n",
       "embeddings = HuggingFaceEmbeddings(\n",
       "            model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
       "            model_kwargs=model_kwargs,\n",
       "            show_progress=False,\n",
       "        )\n",
       "\n",
       "# 次に、Milvus ベクトルストアから関連データを取得するためのリトリーバーを定義します\n",
       "retriever = MilvusRetrieverWithScoreThreshold(\n",
       "            embedding_function=embeddings,\n",
       "            collection_name=\"california_driver_handbook_1_0\",\n",
       "            collection_description=\"\",\n",
       "            collection_properties=None,\n",
       "            connection_args={\n",
       "                \"host\": \"vectordb.milvus.svc.cluster.local\",\n",
       "                \"port\": 19530,\n",
       "            },\n",
       "            score_threshold=0.7, # 閾値を 0.7 に設定\n",
       "            max_result_count=5,  # 最大 5 件の結果を返す\n",
       "        )"
      ]
     }
    ]
   }   