apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: migrated-gpu
    opendatahub.io/apiProtocol: REST
    opendatahub.io/template-display-name: vLLM Serving Runtime
    opendatahub.io/template-name: vllm-serving-runtime
    openshift.io/display-name: vllm-serving-runtime
  labels:
    opendatahub.io/dashboard: "true"
  name: llama-3-elyza-jp-8b-vllm
  namespace: user1
spec:
  containers:
  - args:
    - --trust-remote-code
    - --model
    - /mnt/models/
    - --download-dir
    - /models-cache
    - --port
    - "8080"
    env:
    - name: NUMBA_CACHE_DIR
      value: /tmp
    image: vllm/vllm-openai:latest
    name: kserve-container
    ports:
    - containerPort: 8080
      protocol: TCP
    resources:
      limits:
        cpu: "4"
        memory: 8Gi
      requests:
        cpu: "1"
        memory: 2Gi
    volumeMounts:
    - name: cache
      mountPath: /.cache
    - name: config
      mountPath: /.config
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: pytorch
  volumes:
  - name : cache
    emptyDir: {}          
  - name : config
    emptyDir: {}    
